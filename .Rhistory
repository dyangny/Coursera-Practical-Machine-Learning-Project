# Count the number of NA values for each field
NAValues <- training[, lapply(.SD, function(x) sum(is.na(x)))]
str(NAValues)
# Seems like many columns have at least 19216 NA Values
# Proceed to remove those columns of data since they will not be useful for prediction
NAValues <- melt(NAValues)
setkey(NAValues, "value")
NAValues <- NAValues[list(0)]
NAValues <- as.character(NAValues[["variable"]])
# Reminding ourselves that this vector will contain those variables we want to keep
# We need to add in the mismatched variable from the test dataset
NAValues <- c(NAValues, names(testing)[mismatchVarInd])
training[, names(training)[!(names(training) %in% NAValues)] := NULL]
# Remove some additional columns because they contain user data and are not helpful
names(training)[1:7]
training[, names(training)[1:7] := NULL]
str(training)
# Check for non zero variance covariates
nsv <- nearZeroVar(training, saveMetrics = T)
nsv
# Nothing to eliminate
# Convert the classe column to factor
training[, "classe" := as.factor(classe)]
# Do the same for the test set
testing[, names(testing)[!(names(testing) %in% NAValues)] := NULL]
testing[, names(testing)[1:7] := NULL]
# Search for the best 'mtry' using 10 fold cross validation
mtryGrid <- 1:52
set.seed(3)
seeds <- c(replicate(10, sample.int(1000, length(mtryGrid)), simplify = F), sample.int(1000, 1))
modelRF <- train(classe ~ .,
data = training,
method = "rf",
trControl = trainControl(method = "cv", seeds = seeds),
tuneGrid = data.frame(mtry = mtryGrid))
# Load required packages
sapply(c('data.table',
'magrittr',
'doParallel',
'foreach',
'caret'),
require,
character.only = T)
# Setup up parallel processing
cl <-
makeCluster(detectCores() - 1) %T>%
registerDoParallel()
# Read in the training and test datasets
training <- fread("./Raw Data/pml-training.csv",
na.strings = c("", "NA", "#DIV/0!"))
testing <- fread("./Raw Data/pml-testing.csv",
na.strings = c("", "NA", "#DIV/0!"))
identical(names(testing), names(training))
setdiff((names(testing), names(training))
?setdiff
mismatchVarInd <- setdiff(names(testing), names(training))
names(testing)[mismatchVarInd]
names(training)[mismatchVarInd]
mismatchVarInd <- which(!(names(testing) %in% names(training)))
names(testing)[mismatchVarInd]
names(training)[mismatchVarInd]
identical(names(testing)[1:159], names(testing)[1:159])
NAValues <- training[, lapply(.SD, function(x) sum(is.na(x)))]
View(NAValues)
str(NAValues)
commonVariables <- intersect(names(testing), names(training))
# Read in the training and test datasets
training <- fread("./Raw Data/pml-training.csv",
na.strings = c("", "NA", "#DIV/0!"))
testing <- fread("./Raw Data/pml-testing.csv",
na.strings = c("", "NA", "#DIV/0!"))
nsv <- nearZeroVar(training, saveMetrics = T)
nsv
NAValues <- training[, lapply(.SD, function(x) sum(is.na(x)))]
NAValues <- melt(NAValues)
View(NAValues)
setkey(NAValues, "value")
NAValues <- NAValues[list(0)]
View(NAValues)
NAValues <- as.character(NAValues[["variable"]])
nsv
class(nsv)
setDT(nsv)
nrow(nsv[nsv])
nrow(nsv[nsv == T])
nrow(nsv[nsv == 'TRUE'])
sapply(nsv, class)
sum(nsv[['nsv']])
nsv[['nsv']]
names(nsv)
nzv <- nsv
rm(nsv)
sum(nzv[['nzv']])
commonVariables <- intersect(names(training), names(testing))
stopCluster(cl)
rm(list=ls())
# Setup up parallel processing
cl <-
makeCluster(detectCores() - 1) %T>%
registerDoParallel()
# Read in the training and test datasets
training <- fread("./Raw Data/pml-training.csv",
na.strings = c("", "NA", "#DIV/0!"))
testing <- fread("./Raw Data/pml-testing.csv",
na.strings = c("", "NA", "#DIV/0!"))
commonVariables <- intersect(names(training), names(testing))
setdiff(names(training), commonVariables)
lapply(list(training, testing),
function(x) {
setdiff(names(x), commonVariables)
})
install.packages('lubridate')
install.packages('stringr')
install.packages("stringr")
# Load required packages
sapply(c('data.table',
'magrittr',
'stringr',
'doParallel',
'foreach',
'caret'),
require,
character.only = T)
# Setup up parallel processing
cl <-
makeCluster(detectCores() - 1) %T>%
registerDoParallel()
# Read in the training and test datasets
training <- fread("./Raw Data/pml-training.csv",
na.strings = c("", "NA", "#DIV/0!"))
testing <- fread("./Raw Data/pml-testing.csv",
na.strings = c("", "NA", "#DIV/0!"))
?grep
commonVariables <- intersect(names(training), names(testing))
lapply(list(training, testing),
function(x) {
differingVariable <- setdiff(names(x), commonVariables)
differingVariableIndex <- grep(differingVariable, names(x))
cat(paste0('Differing variable named "', differingVariable,
'" at column ', differingVariableIndex))
})
lapply(list(training, testing),
function(x) {
differingVariable <- setdiff(names(x), commonVariables)
differingVariableIndex <- grep(differingVariable, names(x))
cat(paste0('Differing variable named "', differingVariable,
'" at column ', differingVariableIndex, '.\n'))
})
sapply(list(training, testing),
function(x) {
differingVariable <- setdiff(names(x), commonVariables)
differingVariableIndex <- grep(differingVariable, names(x))
message(paste0('Differing variable named "', differingVariable,
'" at column ', differingVariableIndex, '.\n'))
})
sapply(list(training, testing),
function(x) {
differingVariable <- setdiff(names(x), commonVariables)
differingVariableIndex <- grep(differingVariable, names(x))
return(message(paste0('Differing variable named "', differingVariable,
'" at column ', differingVariableIndex, '.\n')))
})
str(training)
19622-19216
list(0)
NAValues <- training[, lapply(.SD, function(x) sum(is.na(x)))]
str(NAValues)
NAValues <- melt(NAValues)
View(NAValues)
setkey(NAValues, "value")
NAValues <- NAValues[list(0)]
NAValues <- as.character(NAValues[["variable"]])
NAValues <- training[, lapply(.SD, function(x) sum(is.na(x)))]
str(NAValues)
NAValues <- training[, lapply(.SD, function(x) sum(is.na(x)))]
str(NAValues)
NAValues <- melt(NAValues)
setkey(NAValues, "value")
NAValues <- training[, lapply(.SD, function(x) sum(is.na(x)))]
str(NAValues)
View(NAValues)
NAValues <- melt(NAValues)
setkey(NAValues, "value")
NAValues <- NAValues[!.(0)]
View(NAValues)
NAValues <- as.character(NAValues[["variable"]])
?subset
NAValues <-
training[, lapply(.SD, function(x) sum(is.na(x)))] %T>%
str() %>%
melt()
View(NAValues)
NAValues <-
training[, lapply(.SD, function(x) sum(is.na(x)))] %T>%
str() %>%
melt() %T>%
subset(value != 0)
NAValues <-
training[, lapply(.SD, function(x) sum(is.na(x)))] %T>%
str() %>%
melt() %>%
subset(value != 0)
View(NAValues)
NAValues <-
training[, lapply(.SD, function(x) sum(is.na(x)))] %T>%
str() %>%
melt() %>%
subset(value != 0) %>%
as.character(.[[1]])
NAValues <-
training[, lapply(.SD, function(x) sum(is.na(x)))] %T>%
str() %>%
melt() %>%
subset(value != 0)
NAValues <-
training[, lapply(.SD, function(x) sum(is.na(x)))] %T>%
str() %>%
melt() %>%
subset(value != 0) %>%
.[[1]] %>%
as.character()
training[, NAValues := NULL]
training <- fread("./Raw Data/pml-training.csv",
na.strings = c("", "NA", "#DIV/0!"))
testing <- fread("./Raw Data/pml-testing.csv",
na.strings = c("", "NA", "#DIV/0!"))
# The test and train datasets do not have identical variable names
# They differ on the 160th variable name
commonVariables <- intersect(names(training), names(testing))
sapply(list(training, testing),
function(x) {
differingVariable <- setdiff(names(x), commonVariables)
differingVariableIndex <- grep(differingVariable, names(x))
message(paste0('Differing variable named "', differingVariable,
'" at column ', differingVariableIndex, '.\n'))
})
NAValues <-
training[, lapply(.SD, function(x) sum(is.na(x)))] %>%
melt() %>%
subset(value != 0) %>%
.[[1]] %>%
as.character() %T>%
training[, intersect(names(training), .) := NULL]
training[, intersect(names(training), NAValues) := NULL]
View(training)
names(training)[1:7]
userDataVariables <- names(training)[1:7]
training[, intersect(names(training), userDataVariables) := NULL]
nzv <- nearZeroVar(training, saveMetrics = T)
nzv
training[, "classe" := as.factor(classe)]
testing[, intersect(names(.), c(userDataVariables, NAValues))] := NULL]
testing[, intersect(names(.), c(userDataVariables, NAValues)) := NULL]
testing[, intersect(names(testing), c(userDataVariables, NAValues)) := NULL]
mtryGrid <- 1:52
set.seed(3)
seeds <- c(replicate(10, sample.int(1000, length(mtryGrid)), simplify = F), sample.int(1000, 1))
modelRF <- train(classe ~ .,
data = training,
method = "rf",
trControl = trainControl(method = "cv", seeds = seeds),
tuneGrid = data.frame(mtry = mtryGrid))
require(e1071)
install.packages('e1071')
require(e1071)
mtryGrid <- 1:52
set.seed(3)
seeds <- c(replicate(10, sample.int(1000, length(mtryGrid)), simplify = F), sample.int(1000, 1))
modelRF <- train(classe ~ .,
data = training,
method = "rf",
trControl = trainControl(method = "cv", seeds = seeds),
tuneGrid = data.frame(mtry = mtryGrid))
Sys.time()
modelRF$times$everything
5828.74/60
5828.74/60/60
plot(modelRF)
mtryBest <- modelRF$bestTune$mtry
mtryBestAccuracy <- modelRF$results[9, ]
mtryBestError <- (1 - mtryBestAccuracy[1, 2])*100
seeds <- as.list(sample.int(1000, 11))
modelRFFinal <- train(.outcome ~ .,
data = training,
method = "rf",
trControl = trainControl(method = "cv", seeds = seeds),
tuneGrid = data.frame(mtry = mtryBest))
modelRFFinal$finalModel
modelRFFinal$times$everything
modelRFFinalAccuracy <- modelRFFinal$results
modelRFFinalError <- (1 - modelRFFinalAccuracy[1, 2])*100
seeds <- as.list(sample.int(1000, 11))
names(training)
seeds <- as.list(sample.int(1000, 11))
modelRFFinal <- train(classe ~ .,
data = training,
method = "rf",
trControl = trainControl(method = "cv", seeds = seeds),
tuneGrid = data.frame(mtry = mtryBest))
modelRFFinal$finalModel
modelRFFinal$times$everything
modelRFFinalAccuracy <- modelRFFinal$results
modelRFFinalError <- (1 - modelRFFinalAccuracy[1, 2])*100
View(modelRFFinalAccuracy)
# Stop the cluster
stopCluster(cl)
# Load required packages
sapply(c('data.table',
'magrittr',
'stringr',
'doParallel',
'foreach',
'caret'),
require,
character.only = T)
# Setup up parallel processing
cl <-
makeCluster(detectCores() - 1) %T>%
registerDoParallel()
# Read in the training and test datasets
training <- fread("./Raw Data/pml-training.csv",
na.strings = c("", "NA", "#DIV/0!"))
testing <- fread("./Raw Data/pml-testing.csv",
na.strings = c("", "NA", "#DIV/0!"))
# The test and train datasets do not have identical variable names
# They differ on the 160th variable name
commonVariables <- intersect(names(training), names(testing))
sapply(list(training, testing),
function(x) {
differingVariable <- setdiff(names(x), commonVariables)
differingVariableIndex <- grep(differingVariable, names(x))
message(paste0('Differing variable named "', differingVariable,
'" at column ', differingVariableIndex, '.\n'))
})
# Count the number of NA values for each field
# Seems like many columns have either no NA values or at least 19216 NA values
# Proceed to remove those columns of data with NA values since they will not be useful for prediction
NAValues <-
training[, lapply(.SD, function(x) sum(is.na(x)))] %>%
melt() %>%
subset(value != 0) %>%
.[[1]] %>%
as.character()
training[, intersect(names(training), NAValues) := NULL]
# Remove some additional columns because they contain user data and are not helpful
userDataVariables <- names(training)[1:7]
training[, intersect(names(training), userDataVariables) := NULL]
# Check for non zero variance covariates
nzv <- nearZeroVar(training, saveMetrics = T)
nzv
# Nothing to eliminate
# Convert the classe column to factor
training[, "classe" := as.factor(classe)]
# Do the same for the test set
testing[, intersect(names(testing), c(userDataVariables, NAValues)) := NULL]
mtryGrid <- seq(1, 9, 2)
set.seed(3)
seeds <- c(replicate(10, sample.int(1000, length(mtryGrid)), simplify = F), sample.int(1000, 1))
modelRF <- train(classe ~ .,
data = training,
method = "rf",
trControl = trainControl(method = "cv", seeds = seeds),
tuneGrid = data.frame(mtry = mtryGrid))
modelRF$times$everything
482.98/60
plot(modelRF)
mtryBest <- modelRF$bestTune$mtry
modelRF$results
class(modelRF$results)
mtryBestAccuracy <-
modelRF$results %>%
subset(mtry == mtryBest)
View(mtryBestAccuracy)
View(mtryBestAccuracy)
mtryBestError <- (1 - mtryBestAccuracy[1, 2])*100
seeds <- as.list(sample.int(1000, 11))
modelRFFinal <- train(classe ~ .,
data = training,
method = "rf",
trControl = trainControl(method = "cv", seeds = seeds),
tuneGrid = data.frame(mtry = mtryBest))
modelRFFinal$finalModel
modelRFFinal$times$everything
modelRFFinalAccuracy <- modelRFFinal$results
modelRFFinalError <- (1 - modelRFFinalAccuracy[1, 2])*100
require(xgboost)
?xgb.train
mtryGrid <- expand.grid(nrounds = seq(100, 500, 100),
max_depth = seq(1, 5, 1),
eta = 0.3,
gamma = 0.01,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 0.7)
set.seed(3)
# Stop the cluster
stopCluster(cl)
rm(list=ls())
# Load required packages
sapply(c('data.table',
'magrittr',
'stringr',
'doParallel',
'foreach',
'caret'),
require,
character.only = T)
# Setup up parallel processing
cl <-
makeCluster(detectCores() - 1) %T>%
registerDoParallel()
# Read in the training and test datasets
training <- fread("./Raw Data/pml-training.csv",
na.strings = c("", "NA", "#DIV/0!"))
testing <- fread("./Raw Data/pml-testing.csv",
na.strings = c("", "NA", "#DIV/0!"))
# The test and train datasets do not have identical variable names
# They differ on the 160th variable name
commonVariables <- intersect(names(training), names(testing))
sapply(list(training, testing),
function(x) {
differingVariable <- setdiff(names(x), commonVariables)
differingVariableIndex <- grep(differingVariable, names(x))
message(paste0('Differing variable named "', differingVariable,
'" at column ', differingVariableIndex, '.\n'))
})
# Count the number of NA values for each field
# Seems like many columns have either no NA values or at least 19216 NA values
# Proceed to remove those columns of data with NA values since they will not be useful for prediction
NAValues <-
training[, lapply(.SD, function(x) sum(is.na(x)))] %>%
melt() %>%
subset(value != 0) %>%
.[[1]] %>%
as.character()
training[, intersect(names(training), NAValues) := NULL]
# Remove some additional columns because they contain user data and are not helpful
userDataVariables <- names(training)[1:7]
training[, intersect(names(training), userDataVariables) := NULL]
# Check for non zero variance covariates
# Nothing to eliminate
nzv <- nearZeroVar(training, saveMetrics = T)
# Convert the classe column to factor
training[, "classe" := as.factor(classe)]
# Do the same for the test set
testing[, intersect(names(testing), c(userDataVariables, NAValues)) := NULL]
# Search for the best 'mtry' using 10 fold cross validation
mtryGrid <- expand.grid(nrounds = seq(100, 500, 100),
max_depth = seq(1, 5, 1),
eta = 0.3,
gamma = 0.01,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 0.7)
set.seed(3)
modelXGB <- train(classe ~ .,
data = training,
method = "xgbTree",
trControl = trainControl(method = "cv"),
tuneGrid = data.frame(mtry = mtryGrid))
mtryGrid <- expand.grid(nrounds = seq(100, 500, 100),
max_depth = seq(1, 5, 1),
eta = 0.3,
gamma = 0.01,
colsample_bytree = 1,
min_child_weight = 1)
set.seed(3)
modelXGB <- train(classe ~ .,
data = training,
method = "xgbTree",
trControl = trainControl(method = "cv"),
tuneGrid = data.frame(mtry = mtryGrid))
set.seed(3)
modelXGB <- train(classe ~ .,
data = training,
method = "xgbTree",
trControl = trainControl(method = "cv"),
tuneGrid = mtryGrid)
plot(modelXGB)
xgbBest <- modelXGB$bestTune
View(xgbBest)
View(xgbBest)
modelXGB$results
as.list(xgbBest)
xgbBestAccuracy <-
modelXGB$results %T>%
setkeyv(names(xgbBest)) %>%
.[as.list(xgbBest)]
xgbBestAccuracy <-
modelXGB$results %T>%
setDT %T>%
setkeyv(names(xgbBest)) %>%
.[as.list(xgbBest)]
View(xgbBestAccuracy)
View(xgbBestAccuracy)
mtryBestError <- (1 - xgbBestAccuracy[['Accuracy']])*100
View(xgbBestAccuracy)
xgbBestError <- (1 - xgbBestAccuracy[['Accuracy']])*100
modelXGBFinal <- train(classe ~ .,
data = training,
method = "xgbTree",
trControl = trainControl(method = "cv"),
tuneGrid = xgbBest)
modelXGBFinal$finalModel
modelXGBFinal$times$everything
modelXGBFinalAccuracy <- modelXGBFinal$results
View(modelXGBFinalAccuracy)
View(modelXGBFinalAccuracy)
modelXGBFinalError <- (1 - modelXGBFinalAccuracy[1, 'Accuracy'])*100
predictions <- predict(modelXGBFinal, testing)
# Stop the cluster
stopCluster(cl)
